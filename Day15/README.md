### ğŸ”¹ Day 15: Data Wrangling â€“ Deep Dive
What is Data Wrangling?

```python
Data wrangling = turning raw, messy, unreliable data into clean, usable data.

```
Real datasets usually have:

Missing values âŒ

Wrong data types âŒ

Outliers âŒ

Inconsistent text âŒ

Duplicate rows âŒ

### ğŸ§  PART 1: Missing Values (Lecture)
1ï¸âƒ£ What are Missing Values?

Missing values appear as:

NaN (Not a Number)

None

Empty strings ""

Special values like -1, 999, "Unknown"

Example:
```python
import pandas as pd

data = {
```
"Age": [22, 25, None, 30],
"Salary": [30000, None, 50000, 70000],
"City": ["Delhi", None, "Mumbai", "Pune"]
}

```python
df = pd.DataFrame(data)
```
df

2ï¸âƒ£ Detect Missing Values
df.isnull()

Count missing values per column:
df.isnull().sum()

Percentage of missing data:
df.isnull().mean() * 100

### ğŸ’¡ Rule of thumb

<5% â†’ safe to drop or fill

5â€“30% â†’ careful imputation

30% â†’ consider removing column

3ï¸âƒ£ Handling Missing Values
A. Drop Missing Values

- âŒ Risky (you lose data)

df.dropna()          # drop rows
df.dropna(axis=1)   # drop columns

Use when:

Dataset is large

Missing rows are very few

B. Fill Missing Values (Imputation)
### ğŸ”¹ Mean (for numerical data)
df["Age"].fillna(df["Age"].mean(), inplace=True)

Use when:

Data is symmetric

No extreme outliers

### ğŸ”¹ Median (best for salary, income)
df["Salary"].fillna(df["Salary"].median(), inplace=True)

âœ… Robust against outliers

### ğŸ”¹ Mode (for categorical)
df["City"].fillna(df["City"].mode()[0], inplace=True)

### ğŸ”¹ Forward / Backward Fill (time-series)
df.fillna(method="ffill")
df.fillna(method="bfill")

### ğŸ”¹ Custom Value
df["City"].fillna("Unknown", inplace=True)

4ï¸âƒ£ Advanced Imputation (Conceptual)

(Youâ€™ll use these later in ML)

KNN Imputer

Regression-based imputation

ML-based imputation

### ğŸ§  PART 2: Outliers (Lecture)
1ï¸âƒ£ What are Outliers?

```python
Outliers = values far away from normal data

```
Example:

Salary: 30k, 35k, 40k, 45k, 2,00,000 âŒ

Outliers can:

Distort mean

Break ML models

Reduce accuracy

2ï¸âƒ£ Detect Outliers
A. Using IQR (Most Common)
```python
Q1 = df["Salary"].quantile(0.25)
Q3 = df["Salary"].quantile(0.75)

IQR = Q3 - Q1

lower = Q1 - 1.5 * IQR
upper = Q3 + 1.5 * IQR

outliers = df[(df["Salary"] < lower) | (df["Salary"] > upper)]
```
outliers

B. Using Z-Score
```python
from scipy.stats import zscore

df["z_score"] = zscore(df["Salary"])
```
df[df["z_score"].abs() > 3]

3ï¸âƒ£ Handling Outliers
### ğŸ”¹ Remove
```python
df = df[(df["Salary"] >= lower) & (df["Salary"] <= upper)]

```
Use when:

Data error

Extremely rare values

### ğŸ”¹ Cap (Winsorization)
```python
df["Salary"] = df["Salary"].clip(lower, upper)

```
Best for:

Financial data

### ğŸ”¹ Keep Them

Keep if:

Fraud detection

Rare events matter

### ğŸ§  PART 3: Type Conversions (Lecture)
1ï¸âƒ£ Why Data Types Matter?

ML models only understand numbers.

```python
Wrong types = broken pipeline âŒ

```
2ï¸âƒ£ Check Data Types
df.dtypes

3ï¸âƒ£ Convert Types
### ğŸ”¹ String â†’ Integer
```python
df["Age"] = df["Age"].astype(int)

```
### ğŸ”¹ String â†’ Float
```python
df["Salary"] = df["Salary"].astype(float)

```
### ğŸ”¹ Object â†’ Category
```python
df["City"] = df["City"].astype("category")

```
Memory efficient âœ”ï¸

### ğŸ”¹ Date Conversion
```python
df["Date"] = pd.to_datetime(df["Date"])

```
### ğŸ”¹ Extract Date Features
```python
df["Year"] = df["Date"].dt.year
df["Month"] = df["Date"].dt.month

```
### ğŸ§  PART 4: PRACTICE â€“ Handle a Messy Dataset
Messy Dataset Example
```python
data = {
```
"Age": ["22", "25", None, "thirty"],
"Salary": ["30000", "?", "50000", "70000"],
"City": ["delhi", "Delhi", "MUMBAI", None]
}

```python
df = pd.DataFrame(data)

```
Step-by-Step Cleaning
1ï¸âƒ£ Replace invalid values
df.replace("?", None, inplace=True)

2ï¸âƒ£ Standardize text
```python
df["City"] = df["City"].str.lower().str.strip()

```
3ï¸âƒ£ Fix numeric columns
```python
df["Age"] = pd.to_numeric(df["Age"], errors="coerce")
df["Salary"] = pd.to_numeric(df["Salary"], errors="coerce")

```
4ï¸âƒ£ Handle missing
df["Age"].fillna(df["Age"].median(), inplace=True)
df["Salary"].fillna(df["Salary"].median(), inplace=True)
df["City"].fillna(df["City"].mode()[0], inplace=True)

âœ… Final Clean Data
df

### ğŸ§  PART 5: PROJECT â€“ Complete Data Cleaning Workflow
### ğŸ”¹ Real-World Workflow (Industry Style)
1ï¸âƒ£ Load Data
```python
df = pd.read_csv("dataset.csv")

```
2ï¸âƒ£ Initial Inspection
df.head()
df.info()
df.describe()

3ï¸âƒ£ Remove Duplicates
df.drop_duplicates(inplace=True)

4ï¸âƒ£ Handle Missing Values
df.fillna(df.median(), inplace=True)

5ï¸âƒ£ Handle Outliers
```python
for col in df.select_dtypes(include="number"):
    Q1 = df[col].quantile(0.25)
    Q3 = df[col].quantile(0.75)
    IQR = Q3 - Q1
    df[col] = df[col].clip(Q1 - 1.5*IQR, Q3 + 1.5*IQR)

```
6ï¸âƒ£ Fix Data Types
```python
df["date"] = pd.to_datetime(df["date"])

```
7ï¸âƒ£ Final Check
df.isnull().sum()
df.dtypes

### ğŸ§  Key Takeaways (VERY IMPORTANT)

âœ… Data cleaning takes 60â€“70% of ML time
âœ… Median > Mean for real-world data
âœ… Always inspect before cleaning
âœ… Never blindly delete data
```python
âœ… Clean data = better model without tuning
```